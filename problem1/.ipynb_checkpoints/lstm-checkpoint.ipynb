{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125419, 179)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data_loader import DataLoader\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from ml_util import *\n",
    "\n",
    "dl = DataLoader('../data/')\n",
    "x, y = dl.x, dl.y\n",
    "data = np.hstack([x,np.reshape(y, (-1,1))])\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79819, 32, 179)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_length = 32\n",
    "batch_size =512\n",
    "def GetSamplePool(data, seq_length=seq_length):\n",
    "    r = []\n",
    "    for i in range(len(data)-seq_length):\n",
    "        if np.isnan(data[i:i+seq_length]).sum() == 0 and np.isinf(data[i:i+seq_length]).sum() == 0:\n",
    "            r.append(data[i:i+seq_length])\n",
    "    return np.array(r)\n",
    "            \n",
    "data_pool = GetSamplePool(data)\n",
    "data_pool.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((51084, 32, 178),\n",
       " (12771, 32, 178),\n",
       " (15964, 32, 178),\n",
       " (51084, 1),\n",
       " (12771, 1),\n",
       " (15964, 1))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x,  test_x, train_y, test_y = train_test_split(data_pool[:,:,:-1], data_pool[:,-1,-1], test_size=0.2, random_state=1)\n",
    "train_x, valid_x, train_y,  valid_y = train_test_split(train_x, train_y, test_size=0.2, random_state=1)\n",
    "train_y = np.reshape(train_y, (-1, 1))\n",
    "valid_y = np.reshape(valid_y, (-1, 1))\n",
    "test_y = np.reshape(test_y, (-1, 1))\n",
    "train_x.shape, valid_x.shape, test_x.shape, train_y.shape, valid_y.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in greater\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/root/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/_encoders.py:363: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features=None, categories=None,\n",
       "       dtype=<type 'numpy.float64'>, handle_unknown='error',\n",
       "       n_values='auto', sparse=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit(np.reshape(y>0, (-1,1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /root/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling __init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50)                45800     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 30)                1530      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                620       \n",
      "_________________________________________________________________\n",
      "out (Dense)                  (None, 2)                 42        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 47,992\n",
      "Trainable params: 47,992\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# lstm model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import activations\n",
    "import tensorflow as tf\n",
    "\n",
    "def focal_loss(pred, real,gamma=2):\n",
    "    L=-real*((1-pred)**gamma)*tf.log(pred+1e-15)\n",
    "    L=tf.reduce_sum(L,axis=1)\n",
    "    return L\n",
    "\n",
    "def lstm_model():\n",
    "    model = keras.Sequential()\n",
    "    #model.add(layers.Dense(50, activation='relu', input_dim = dl.x.shape[1])),#, return_sequences=True))\n",
    "    #model.add(layers.Flatten())\n",
    "    model.add(layers.LSTM(50, input_shape=(seq_length, dl.x.shape[1]),return_sequences=False))\n",
    "    model.add(layers.Dense(30, activation='relu'))\n",
    "    model.add(layers.Dense(20, activation='relu'))\n",
    "    model.add(layers.Dense(2, name='out'))\n",
    "    model.add(layers.Activation('sigmoid'))\n",
    "    model.compile(optimizer=keras.optimizers.Adam(lr=1e-3),\n",
    "        #optimizer=keras.optimizers.SGD(1e-2),\n",
    "        loss=keras.losses.CategoricalCrossentropy(),\n",
    "        #loss = focal_loss,\n",
    "        metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = lstm_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom numpy import random\\nfrom sklearn.preprocessing import OneHotEncoder\\nohe = OneHotEncoder()\\nohe.fit(np.reshape(dl.y>0, (-1,1)))\\n\\ndef GenSeqSamples(x, y, batch_size=64, seq_length=32):\\n    count = 0\\n    rx, ry, index = [], [], []\\n    while count < batch_size:\\n        i = random.randint(len(x)-seq_length) #, size=batch_size)\\n        tx, ty = x[i:i+seq_length], y[i+seq_length-1]\\n        if np.isnan(tx).sum() != 0 or np.isnan(ty).sum() != 0:\\n            continue\\n        rx.append(tx)\\n        ry.append(ohe.transform(np.reshape(ty>0,(-1, 1))).toarray()[0])\\n        index.append(i)\\n        count += 1\\n    return np.array(rx), np.array(ry), np.array(index)\\n    \\n# filter data: remove those nan and inf\\ndef Filter(x, y):\\n    nan_cond = (~np.isnan(y)) & (np.isnan(x).sum(axis=1) == 0)\\n    inf_cond = (~np.isinf(y)) & (np.isinf(x).sum(axis=1) == 0)\\n    return x[nan_cond & inf_cond], y[nan_cond * inf_cond]\\n    #print(x.shape, y_shape, rx.shape, ry.shape)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from numpy import random\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit(np.reshape(dl.y>0, (-1,1)))\n",
    "\n",
    "def GenSeqSamples(x, y, batch_size=64, seq_length=32):\n",
    "    count = 0\n",
    "    rx, ry, index = [], [], []\n",
    "    while count < batch_size:\n",
    "        i = random.randint(len(x)-seq_length) #, size=batch_size)\n",
    "        tx, ty = x[i:i+seq_length], y[i+seq_length-1]\n",
    "        if np.isnan(tx).sum() != 0 or np.isnan(ty).sum() != 0:\n",
    "            continue\n",
    "        rx.append(tx)\n",
    "        ry.append(ohe.transform(np.reshape(ty>0,(-1, 1))).toarray()[0])\n",
    "        index.append(i)\n",
    "        count += 1\n",
    "    return np.array(rx), np.array(ry), np.array(index)\n",
    "    \n",
    "# filter data: remove those nan and inf\n",
    "def Filter(x, y):\n",
    "    nan_cond = (~np.isnan(y)) & (np.isnan(x).sum(axis=1) == 0)\n",
    "    inf_cond = (~np.isinf(y)) & (np.isinf(x).sum(axis=1) == 0)\n",
    "    return x[nan_cond & inf_cond], y[nan_cond * inf_cond]\n",
    "    #print(x.shape, y_shape, rx.shape, ry.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xt, yt, _ = GenSeqSamples(x_train, y_train)#, 1, 1)\n",
    "#xt.shape, yt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 51084 samples, validate on 12771 samples\n",
      "WARNING:tensorflow:From /root/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/math_grad.py:1250: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "earlystop_callback = EarlyStopping(\n",
    "  monitor='val_loss', min_delta=0.0001,\n",
    "  patience=10)\n",
    "#ohe.transform(train_y).toarray().shape\n",
    "model.fit(train_x, ohe.transform(train_y>0).toarray(), batch_size = batch_size,\n",
    "          validation_data=[valid_x, ohe.transform(valid_y>0).toarray()],\n",
    "          callbacks=[earlystop_callback], epochs=30)\n",
    "    #print(model.predict(xt[:num]), yt[:num], GetMidRes(model, 'out', xt[:num]))\n",
    "    #if np.isnan(model.predict(xt[:num])).sum() > 0:\n",
    "    #print(xt[:num], yt[:num], index, model.predict(xt[:num]))\n",
    "    #raw_input()\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = model.history.history\n",
    "train_loss, valid_loss  = h['loss'], h['val_loss']\n",
    "plt.plot(train_loss, label='train loss')\n",
    "plt.plot(valid_loss, label='valid loss')\n",
    "plt.title('loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_util import *\n",
    "pred = model.predict(test_x)\n",
    "pred.shape\n",
    "(pred.argmax(axis=1) == 0).mean()\n",
    "a = ClsReport(model,  test_y> 0, pred.argmax(axis=1), binary=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
