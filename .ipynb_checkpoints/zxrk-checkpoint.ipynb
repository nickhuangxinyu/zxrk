{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 2] No such file or directory: '../data/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-61e13779d911>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mdf_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../data/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'tick'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 2] No such file or directory: '../data/'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from ml_util import *\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cols = ['UpdateTime', 'UpdateMillisec', 'InstrumentID', 'Volume', 'LastPrice', 'OpenInterest', 'PreSettlementPrice',\n",
    "        'PreClosePrice', 'PreOpenInterest', 'OpenPrice', 'HighestPrice', 'LowestPrice', 'ClosePrice', 'SettlementPrice',\n",
    "        'UpperLimitPrice', 'LowerLimitPrice', 'BidPrice1', 'AskPrice1', 'BidVolume1', 'AskVolume1', 'Turnover', 'TradingDay', 'LocalTime']\n",
    "\n",
    "def MACD(mp, window_size):\n",
    "    ema_long = mp.ewm(span=2*window_size, adjust=False).mean()\n",
    "    ema_short = mp.ewm(span=window_size, adjust=False).mean()\n",
    "    diff = ema_long - ema_short\n",
    "    dea = diff.ewm(span=math.ceil(0.8*window_size), adjust=False).mean()\n",
    "    return 2*(diff - dea)\n",
    "\n",
    "contract_size = 300\n",
    "time_set = [1,3,5,10,20,30,50, 100, 200]\n",
    "x_col = []\n",
    "y_col = 'yield_diff'\n",
    "df_list = []\n",
    "data_path = '../data/'\n",
    "for f in os.listdir(data_path):\n",
    "    if 'tick' not in f:\n",
    "        continue\n",
    "    df = pd.read_csv(data_path+f, header=0, names = cols, dtype={'UpdateTime':str, 'UpdateMillisec':str})\n",
    "    df['mp'] = (df['BidPrice1'] + df['AskPrice1'])/2\n",
    "    df['wp'] = (df['BidPrice1'] * df['BidVolume1'] + df['AskPrice1'] * df['AskVolume1']) / (df['BidVolume1'] + df['AskVolume1'])\n",
    "    df['rwp'] = (df['BidPrice1'] * df['AskVolume1'] + df['AskPrice1'] * df['BidVolume1']) / (df['BidVolume1'] + df['AskVolume1'])\n",
    "    df['atp'] = df['Turnover'].diff(1) / df['Volume'].diff(1) / contract_size\n",
    "    df['wp-mp'] = df['wp'] - df['mp']\n",
    "    df['rwp-mp'] = df['rwp'] - df['mp']\n",
    "    df['atp-mp'] = df['atp'] - df['mp']\n",
    "    df['r10'] = df['mp'].diff(-10) / df['mp']\n",
    "    df = df[df['UpdateTime'] > '09']\n",
    "    df['time_str'] = df['UpdateTime'] + df['UpdateMillisec']\n",
    "    df_x = df[df['InstrumentID'] == 'IF2009']\n",
    "    df_y = df[df['InstrumentID'] == 'IF2008']\n",
    "    df = pd.merge(df_x, df_y, on=['time_str'], how='inner')\n",
    "    df['mid_delta'] = df['mp_x'] - df['mp_y']\n",
    "    for t in time_set:\n",
    "        for s in ['x', 'y']:\n",
    "            df['mid_diff_%s_%d' % (s, t)] = df['mp_%s' % (s)].diff(t)\n",
    "            x_col.append('mid_diff_%s_%d' % (s, t))\n",
    "            df['openinterest_diff_%s_%d' % (s, t)] = df['OpenInterest_%s' % (s)].diff(t)\n",
    "            x_col.append('openinterest_diff_%s_%d' % (s, t))\n",
    "            df['mid_delta_diff_%d' % (t)] = df['mid_delta'].diff(t)\n",
    "            x_col.append('mid_delta_diff_%d' % (t))\n",
    "            df['wp-mp_diff_%s_%d' % (s, t)] = df['wp-mp_%s' %(s)].diff(t)\n",
    "            x_col.append('wp-mp_diff_%s_%d' % (s, t))\n",
    "            df['atp-mp_diff_%s_%d' % (s, t)] = df['atp-mp_%s' %(s)].diff(t)\n",
    "            x_col.append('atp-mp_diff_%s_%d' % (s, t))\n",
    "            df['rwp-mp_diff_%s_%d' % (s, t)] = df['rwp-mp_%s' %(s)].diff(t)\n",
    "            x_col.append('rwp-mp_diff_%s_%d' % (s, t))\n",
    "        if t > 1:\n",
    "            df['mid_delta_gap_%d'%(t)] = (df['mid_delta'] - df['mid_delta'].rolling(t).mean())/df['mid_delta'].rolling(t).std()\n",
    "            x_col.append('mid_delta_gap_%d'%(t))\n",
    "            df['macd_mid_%s_%d' % (s, t)] = MACD(df['mp_%s'%(s)], t)\n",
    "            x_col.append('macd_mid_%s_%d' % (s, t))\n",
    "    df_list.append(df)\n",
    "\n",
    "x_col += ['wp-mp_x', 'atp-mp_x', 'rwp-mp_x']\n",
    "x_col += ['wp-mp_y', 'atp-mp_y', 'rwp-mp_y']\n",
    "    \n",
    "df = reduce(lambda x,y: x.append(y), df_list)\n",
    "df[y_col] = df['r10_x'] - df['r10_y']\n",
    "\n",
    "#df = df.reset_index()\n",
    "#nan_index = df.loc[df.T.isnull().any()].index.tolist()\n",
    "#df = df.loc[~df[x_col].T.isnull().any()]\n",
    "#np.isnan(df[x_col]).sum()\n",
    "x, y = df[x_col].values, df[y_col].values\n",
    "#np.isnan(x).sum()\n",
    "#df\n",
    "#x = Imputer(missing_values='NaN', strategy='mean',axis=0).fit_transform(x)\n",
    "x = RobustScaler().fit_transform(x)\n",
    "x_col_map = {t:i for i, t in enumerate(x_col)} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('y distribution')\n",
    "plt.hist(df[y_col])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotFactorCorr(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split date into training_set and test_set\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "code = np.reshape((y>0), (-1, 1))\n",
    "y_onehot= OneHotEncoder().fit(code).transform(code).toarray()\n",
    "test_ratio = 0.2\n",
    "valid_ratio = 0.2\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_ratio, random_state=0)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size = 0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear model\n",
    "#model = linear_model.ElasticNetCV(alphas=[0.0,0.0001, 0.0005, 0.001, 0.01, 0.1, 1, 10], l1_ratio=[0.0, .01, .1, .5, .9, .99, 0.1],  max_iter=5000,  tol=1e-15)\n",
    "#model.fit(x_train, y_train)\n",
    "#print('expr:y=%s*x+%lf, iter=%d' % (str(model.coef_), model.intercept_, model.n_iter_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost\n",
    "model = xgb.XGBClassifier(max_depth=50, learning_rate=0.1, n_estimators=1000, silent=False, objective='binary:logistic', num_classes=2,\n",
    "                          booster='gbtree', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0,\n",
    "                          subsample=1, colsample_bytree=1, colsample_bylevel=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1)\n",
    "eval_set = [(x_train, y_onehot_train>0),(x_valid, y_onehot_valid>0)]\n",
    "model.fit(x_train, y_onehot_train>0, eval_set=eval_set, verbose=True, eval_metric='logloss', early_stopping_rounds=10)\n",
    "y_onehot_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ClsReport(model, y_train>0, model.predict(x_train)>0, binary=True)\n",
    "selected_features = [x_col_map[i] for i in a]\n",
    "x = x[:, a]\n",
    "test_ratio = 0.2\n",
    "valid_ratio = 0.2\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_ratio, random_state=0)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size = 0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import cross_val_score\n",
    "#a = ClsReport(model, y_test>0, y_pred>0, binary=True)\n",
    "#model = xgb.XGBClassifier(max_depth=50, learning_rate=0.1, n_estimators=1000, silent=False, objective='binary:logistic', num_classes=2,\n",
    "                          #booster='gbtree', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0,\n",
    "                          #subsample=1, colsample_bytree=1, colsample_bylevel=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1)\n",
    "#cross_val_score(model, x, y>0, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import random\n",
    "def GetSamples(x, y, batch_size=64, seq_length=32):\n",
    "    count = 0\n",
    "    rx, ry, index = [], [], []\n",
    "    while count < batch_size:\n",
    "        i = random.randint(len(x)-seq_length) #, size=batch_size)\n",
    "        tx, ty = x[i:i+seq_length], y[i+seq_length-1]\n",
    "        if np.isnan(tx).sum() != 0:\n",
    "            continue\n",
    "        rx.append(tx)\n",
    "        ry.append(ty)\n",
    "        index.append(i)\n",
    "        count += 1\n",
    "    return np.array(rx), np.array(ry), np.array(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt, yt, _ = GetSamples(x_train, y_train)#, 1, 1)\n",
    "xt.shape, yt.shape\n",
    "#np.isnan(xt).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import activations\n",
    "\n",
    "batch_size = 128\n",
    "seq_length = 32\n",
    "def lstm_model():\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Dense(300, activation='relu', input_shape=(seq_length, x.shape[1]))),#, return_sequences=True))\n",
    "    model.add(layers.Flatten())\n",
    "    #model.add(layers.LSTM(2, return_sequences=False,name='out'))\n",
    "    model.add(layers.Dense(100, activation='relu'))\n",
    "    model.add(layers.Dense(2, name='out'))\n",
    "    model.add(layers.Activation('sigmoid'))\n",
    "    model.compile(optimizer=keras.optimizers.Adam(lr=1e-2),\n",
    "                 loss=keras.losses.CategoricalCrossentropy(),\n",
    "                 metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = lstm_model()\n",
    "model.summary()\n",
    "\n",
    "#mid_model = Model(inputs=model.input, outputs=model.get_layer('out').output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 1#batch_size\n",
    "xt, yt, index = GetSamples(x_train, y_train, batch_size=batch_size, seq_length=seq_length)\n",
    "for i in range(100):\n",
    "    #print(np.isnan(xt).sum())\n",
    "    print(model.predict(xt[:num]), GetMidRes(model, 'out', xt[:num]))\n",
    "    model.fit(xt[:num], yt[:num])\n",
    "    #print(model.predict(xt[:num]), yt[:num], GetMidRes(model, 'out', xt[:num]))\n",
    "    #if np.isnan(model.predict(xt[:num])).sum() > 0:\n",
    "    print(xt[:num], yt[:num], index, model.predict(xt[:num]))\n",
    "    raw_input()\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in x_col:\n",
    "    df[col].describe()\n",
    "    df[col].plot.box(title=col)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler as ss\n",
    "from sklearn.preprocessing import Imputer\n",
    "df['atp-mp_diff_x_20'].plot.box()\n",
    "plt.show()\n",
    "d = np.reshape(df['atp-mp_diff_x_20'].tolist(), (-1, 1))\n",
    "d = Imputer(missing_values='NaN', strategy='mean',axis=0).fit_transform(d)\n",
    "plt.boxplot(ss().fit_transform(d))\n",
    "plt.show()\n",
    "plt.hist(ss().fit_transform(d))\n",
    "plt.show()\n",
    "ss().fit_transform(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
